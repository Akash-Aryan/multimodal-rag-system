# Multimodal RAG System

A comprehensive Retrieval-Augmented Generation (RAG) system leveraging Large Language Models (LLMs) for **OFFLINE mode** that can ingest, index, and query diverse data formats including documents, images, and voice recordings within a unified semantic retrieval framework.

## ğŸ¯ Project Overview

This system provides:

- **Multimodal Content Processing**: Handle text documents (PDF, DOCX, TXT, MD), images (JPG, PNG, etc.), and audio files (WAV, MP3, etc.)
- **Offline LLM Integration**: Works with local LLMs via Ollama for complete privacy and offline operation
- **Semantic Search**: Vector-based similarity search across all content types
- **Unified Query Interface**: Ask questions about your documents, images, and audio content in natural language
- **Flexible Architecture**: Modular design supporting different embedding models, vector stores, and LLMs

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Document      â”‚    â”‚     Image       â”‚    â”‚     Audio       â”‚
â”‚   Processor     â”‚    â”‚   Processor     â”‚    â”‚   Processor     â”‚
â”‚  (PDF,DOCX,TXT) â”‚    â”‚ (JPG,PNG,etc.)  â”‚    â”‚ (WAV,MP3,etc.)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Embedding Manager        â”‚
                    â”‚ (Text & Multimodal Models)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Vector Store           â”‚
                    â”‚    (FAISS/Chroma)           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Multimodal Retriever      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      LLM Manager            â”‚
                    â”‚   (Ollama Integration)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

### Data Ingestion
- **Text Documents**: PDF, DOCX, TXT, Markdown files
- **Images**: JPG, PNG, GIF with OCR text extraction
- **Audio**: WAV, MP3, M4A with speech-to-text transcription
- **Batch Processing**: Ingest entire directories of mixed content

### Processing Capabilities
- **Text Chunking**: Intelligent text splitting with overlap
- **OCR**: Extract text from images using Tesseract
- **Speech Recognition**: Audio transcription using OpenAI Whisper
- **Metadata Extraction**: File hashes, sizes, processing timestamps

### Search & Retrieval
- **Semantic Search**: Vector similarity search across all content types
- **Hybrid Queries**: Combine text, image, and audio queries
- **Configurable Results**: Adjust similarity thresholds and result counts
- **Source Attribution**: Full traceability to original files

### LLM Integration
- **Offline Operation**: Use local LLMs via Ollama
- **Context-Aware Responses**: RAG-enhanced generation with retrieved context
- **Configurable Models**: Support for different model sizes and parameters

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- [Ollama](https://ollama.ai/) for local LLM inference
- [Tesseract OCR](https://github.com/tesseract-ocr/tesseract) for image text extraction

## ğŸ› ï¸ Installation

### 1. Clone the Repository

```bash
git clone <repository-url>
cd multimodal-rag-system
```

### 2. Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\\Scripts\\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Install System Dependencies

#### Tesseract OCR
- **Ubuntu/Debian**: `sudo apt-get install tesseract-ocr`
- **macOS**: `brew install tesseract`
- **Windows**: Download from [GitHub releases](https://github.com/UB-Mannheim/tesseract/wiki)

#### Ollama
1. Install Ollama from [https://ollama.ai/](https://ollama.ai/)
2. Pull a model: `ollama pull llama2`
3. Verify installation: `ollama list`

### 5. Configuration

```bash
cp .env.example .env
# Edit .env with your specific paths and configuration
```

## ğŸ”§ Configuration

The system uses YAML configuration files located in `config/config.yaml`. Key settings include:

```yaml
# LLM Configuration
llm:
  provider: "ollama"
  model_name: "llama2"
  temperature: 0.7
  max_tokens: 1024

# Vector Store
vector_store:
  provider: "faiss"
  dimension: 384
  persist_directory: "./data/embeddings"

# Document Processing
document_processing:
  chunk_size: 1000
  chunk_overlap: 200
```

## ğŸš€ Quick Start

### Basic Usage

```python
import asyncio
from src.multimodal_rag import MultimodalRAG, RAGConfig

async def main():
    # Initialize the system
    config = RAGConfig.from_yaml("config/config.yaml")
    rag_system = MultimodalRAG(config)
    
    # Ingest documents
    await rag_system.ingest_file("path/to/document.pdf")
    await rag_system.ingest_file("path/to/image.jpg")
    await rag_system.ingest_file("path/to/audio.mp3")
    
    # Or ingest entire directory
    await rag_system.ingest_directory("data/raw")
    
    # Query the system
    result = await rag_system.query(
        "What are the main topics discussed in the documents?",
        query_type="text",
        top_k=5
    )
    
    print(f"Answer: {result['response']}")
    print(f"Sources: {len(result.get('sources', []))} documents")

# Run the async function
asyncio.run(main())
```

### CLI Usage

```bash
# Ingest documents
python scripts/ingest.py --directory data/raw

# Query the system
python scripts/query.py --query "What is discussed in the documents?" --top-k 5

# Check system status
python scripts/status.py
```

## ğŸ“– Advanced Usage

### Custom Configuration

```python
from src.multimodal_rag import RAGConfig, MultimodalRAG

# Create custom configuration
config = RAGConfig(
    llm=LLMConfig(
        model_name="mistral",
        temperature=0.3
    ),
    vector_store=VectorStoreConfig(
        provider="chroma",
        dimension=768
    )
)

rag_system = MultimodalRAG(config)
```

### Batch Processing

```python
# Process multiple files
files = ["doc1.pdf", "image1.jpg", "audio1.wav"]
results = []

for file_path in files:
    result = await rag_system.ingest_file(file_path)
    results.append(result)

# Save index
await rag_system.save_index()
```

### Custom Queries

```python
# Text query
text_result = await rag_system.query(
    "Summarize the key findings",
    query_type="text",
    top_k=3
)

# Image-based query (using OCR text)
image_result = await rag_system.query(
    "path/to/query_image.jpg",
    query_type="image",
    top_k=5
)
```

## ğŸ³ Docker Deployment

```bash
# Build the image
docker-compose build

# Run the system
docker-compose up -d

# Access the API
curl http://localhost:8000/query -X POST -H "Content-Type: application/json" -d '{"query": "What is in the documents?", "top_k": 5}'
```

## ğŸ“ Project Structure

```
multimodal-rag-system/
â”œâ”€â”€ src/multimodal_rag/           # Core package
â”‚   â”œâ”€â”€ ingestion/                # Data ingestion modules
â”‚   â”‚   â”œâ”€â”€ document_processor.py # Text document processing
â”‚   â”‚   â”œâ”€â”€ image_processor.py    # Image processing & OCR
â”‚   â”‚   â””â”€â”€ audio_processor.py    # Audio transcription
â”‚   â”œâ”€â”€ indexing/                 # Indexing and embeddings
â”‚   â”‚   â”œâ”€â”€ vector_store.py       # Vector database interface
â”‚   â”‚   â””â”€â”€ embeddings.py         # Embedding generation
â”‚   â”œâ”€â”€ retrieval/                # Search and retrieval
â”‚   â”‚   â””â”€â”€ retriever.py          # Multimodal retriever
â”‚   â”œâ”€â”€ models/                   # LLM integration
â”‚   â”‚   â””â”€â”€ llm_manager.py        # LLM interface
â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â””â”€â”€ core.py                   # Main RAG system
â”œâ”€â”€ config/                       # Configuration files
â”‚   â””â”€â”€ config.yaml               # Default configuration
â”œâ”€â”€ scripts/                      # Utility scripts
â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”œâ”€â”€ examples/                     # Example usage
â”œâ”€â”€ tests/                        # Test suite
â”œâ”€â”€ data/                         # Data directories
â”‚   â”œâ”€â”€ raw/                      # Raw input files
â”‚   â”œâ”€â”€ processed/                # Processed data
â”‚   â””â”€â”€ embeddings/               # Vector embeddings
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ Dockerfile                    # Container configuration
â”œâ”€â”€ docker-compose.yml            # Multi-container setup
â””â”€â”€ README.md                     # This file
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test categories
pytest tests/test_ingestion.py
pytest tests/test_retrieval.py

# Run with coverage
pytest --cov=src tests/
```

## ğŸ”„ API Reference

### Core Methods

#### `MultimodalRAG.ingest_file(file_path: str) -> Dict`
Ingest a single file into the system.

#### `MultimodalRAG.ingest_directory(directory_path: str) -> List[Dict]`
Ingest all supported files in a directory.

#### `MultimodalRAG.query(query: str, query_type: str = "text", top_k: int = 5) -> Dict`
Query the RAG system and get AI-generated responses.

#### `MultimodalRAG.get_system_stats() -> Dict`
Get system statistics and status information.

## ğŸ› ï¸ Development

### Setting up Development Environment

```bash
# Install development dependencies
pip install -r requirements.txt
pip install -e .

# Install pre-commit hooks
pre-commit install

# Run linting
black src/ tests/
flake8 src/ tests/
mypy src/
```

### Adding New Data Types

1. Create a new processor in `src/multimodal_rag/ingestion/`
2. Update the supported formats in `config.py`
3. Add processing logic to `core.py`
4. Update the embedding manager for new content types
5. Add tests for the new functionality

## ğŸ“Š Performance

### Benchmarks
- **Text Processing**: ~100 pages/minute
- **Image OCR**: ~10 images/minute
- **Audio Transcription**: ~5x real-time (depends on Whisper model)
- **Query Response**: <2 seconds for most queries

### Optimization Tips
- Use GPU acceleration for embedding models
- Adjust chunk sizes based on your content
- Consider using larger Whisper models for better accuracy
- Optimize FAISS index type for your use case

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/new-feature`
3. Make your changes and add tests
4. Run the test suite: `pytest tests/`
5. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) for local LLM inference
- [OpenAI Whisper](https://github.com/openai/whisper) for speech recognition
- [Sentence Transformers](https://www.sbert.net/) for text embeddings
- [FAISS](https://github.com/facebookresearch/faiss) for vector similarity search
- [LangChain](https://github.com/langchain-ai/langchain) for text processing utilities

## ğŸ“¬ Support

For questions, issues, or contributions, please:
- Open an issue on GitHub
- Check the [documentation](docs/)
- Join our community discussions

---

**Built with â¤ï¸ for the open-source community**
